{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0500e407-879c-4165-a8dc-9950a6b18c70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shlomi.fenster/.venv/nemoenv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('/home/shlomi.fenster/notebooks/nemo_stuff/Onboarding/Ensemble/phatgoose')\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "import pytorch_lightning as pl\n",
    "from data_utils.data import MultiRouterDatasetV0\n",
    "from model_utils.models import MultiRouterModelV0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d656f5a3-098d-482a-82e9-b731c0882803",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "dataset_df = pd.read_pickle('router_dataset_v1.pkl')\n",
    "ds = MultiRouterDatasetV0(dataset_df)\n",
    "dl = DataLoader(ds, 32, collate_fn=ds.collate_fn, shuffle=True, drop_last=True)\n",
    "\n",
    "multi_router_model = MultiRouterModelV0(sorted(set(dataset_df['Model'])))\n",
    "\n",
    "logger = pl.loggers.TensorBoardLogger('/home/shlomi.fenster/notebooks/nemo_stuff/Onboarding/Ensemble/')\n",
    "trainer = pl.Trainer(max_epochs=2, max_steps=20, val_check_interval=10, log_every_n_steps=10, logger=logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed9a2b23-8c81-4312-8d90-762032a6ad8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shlomi.fenster/.venv/nemoenv/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "You are using a CUDA device ('NVIDIA L40S') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name        | Type              | Params | Mode \n",
      "----------------------------------------------------------\n",
      "0 | module_dict | ModuleDict        | 17.4 M | train\n",
      "1 | loss        | BCEWithLogitsLoss | 0      | train\n",
      "----------------------------------------------------------\n",
      "17.4 M    Trainable params\n",
      "0         Non-trainable params\n",
      "17.4 M    Total params\n",
      "69.456    Total estimated model params size (MB)\n",
      "802       Modules in train mode\n",
      "0         Modules in eval mode\n",
      "SLURM auto-requeueing enabled. Setting signal handlers.\n",
      "/home/shlomi.fenster/.venv/nemoenv/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 20/5458 [00:04<19:51,  4.56it/s, v_num=3] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=20` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 20/5458 [00:05<25:54,  3.50it/s, v_num=3]\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(multi_router_model, dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9eaaf40-2cf7-42c6-bd6c-ae4d51c57b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_tar_ids = dataset_df['tar_id'].drop_duplicates().sample(100)\n",
    "val_df = dataset_df[dataset_df['tar_id'].isin(val_tar_ids)]\n",
    "\n",
    "val_ds = MultiRouterDatasetV0(val_df)\n",
    "val_dl = DataLoader(val_ds, 64, collate_fn=MultiRouterDatasetV0.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29a334a7-4b79-4363-b348-ed92d2659cc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "SLURM auto-requeueing enabled. Setting signal handlers.\n",
      "/home/shlomi.fenster/.venv/nemoenv/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation DataLoader 0:   3%|▎         | 1/38 [00:00<00:02, 16.97it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shlomi.fenster/.venv/nemoenv/lib/python3.10/site-packages/pytorch_lightning/utilities/data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 64. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "/home/shlomi.fenster/notebooks/nemo_stuff/Onboarding/Ensemble/phatgoose/model_utils/models.py:102: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  self.validation_results_df = pd.concat([self.validation_results_df, batch_val_df])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation DataLoader 0: 100%|██████████| 38/38 [00:05<00:00,  6.47it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shlomi.fenster/.venv/nemoenv/lib/python3.10/site-packages/pytorch_lightning/utilities/data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 15. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">      Validate metric      </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         val_loss          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.6868410110473633     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m     Validate metric     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m        val_loss         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.6868410110473633    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'val_loss': 0.6868410110473633}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.validate(multi_router_model, val_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "46a86147-5a2c-4ef9-9cb0-b20af897c7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "aaa = multi_router_model.validation_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c6e91886-6167-4d8c-b1f1-eb6e18217313",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: ylabel='Frequency'>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjsAAAGdCAYAAAD0e7I1AAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAL6lJREFUeJzt3Xt0FGWe//FP7hCgEwMkDUsICAhECGDQ0IqumkiADKLgWUGE6HJgZIODxAsTfwwKzBgEBS+L4M5y8yjisOtlRQEhXNQhXIxGEMYIGTQwSScMDGmCS8ilfn946LUl3JoO3Xl4v86pc9JVT1V/v1TK/lj9dCfIsixLAAAAhgr2dwEAAACNibADAACMRtgBAABGI+wAAACjEXYAAIDRCDsAAMBohB0AAGA0wg4AADBaqL8LCAT19fUqLS1Vq1atFBQU5O9yAADARbAsSydOnFD79u0VHHzu+zeEHUmlpaWKj4/3dxkAAMALhw4dUocOHc65nbAjqVWrVpJ++sey2Wx+rgYAAFwMl8ul+Ph49+v4uQRM2JkzZ45ycnI0ZcoUvfTSS5KkU6dO6fHHH9eqVatUXV2t9PR0vfbaa4qLi3PvV1JSokmTJmnz5s1q2bKlMjMzlZubq9DQi2/tzFtXNpuNsAMAQBNzoSkoATFBedeuXXr99deVlJTksX7q1Kn68MMPtXr1am3dulWlpaUaMWKEe3tdXZ0yMjJ0+vRpbdu2TStWrNDy5cs1Y8aMK90CAAAIUH4PO1VVVRozZoz++Mc/6pprrnGvr6ys1JIlSzR//nzdeeedSk5O1rJly7Rt2zZt375dkvTJJ59o3759evPNN9W3b18NGTJEs2fP1sKFC3X69Gl/tQQAAAKI38NOVlaWMjIylJaW5rG+oKBANTU1Hut79Oihjh07Kj8/X5KUn5+v3r17e7ytlZ6eLpfLpb17957zOaurq+VyuTwWAABgJr/O2Vm1apW+/PJL7dq166xtTqdT4eHhio6O9lgfFxcnp9PpHvPzoHNm+5lt55Kbm6uZM2deZvUAAKAp8NudnUOHDmnKlCl666231KxZsyv63Dk5OaqsrHQvhw4duqLPDwAArhy/hZ2CggJVVFTohhtuUGhoqEJDQ7V161a98sorCg0NVVxcnE6fPq3jx4977FdeXi673S5JstvtKi8vP2v7mW3nEhER4f7kFZ/AAgDAbH4LO6mpqdqzZ48KCwvdS//+/TVmzBj3z2FhYcrLy3PvU1RUpJKSEjkcDkmSw+HQnj17VFFR4R6zYcMG2Ww2JSYmXvGeAABA4PHbnJ1WrVqpV69eHutatGih1q1bu9ePHz9e2dnZiomJkc1m06OPPiqHw6EBAwZIkgYNGqTExESNHTtWc+fOldPp1PTp05WVlaWIiIgr3hMAAAg8AfOlgg1ZsGCBgoODNXLkSI8vFTwjJCREa9as0aRJk+RwONSiRQtlZmZq1qxZfqwaAAAEkiDLsix/F+FvLpdLUVFRqqysZP4OAABNxMW+fvv9e3YAAAAaE2EHAAAYjbADAACMRtgBAABGC+hPYwHA1arTbz/ydwmX7Ps5Gf4uAWgQd3YAAIDRuLMDAPAJ7kYhUHFnBwAAGI07O4CfNMX/C5b4P2GYpSleh1yDl447OwAAwGiEHQAAYDTCDgAAMBphBwAAGI2wAwAAjEbYAQAARiPsAAAAoxF2AACA0Qg7AADAaIQdAABgNMIOAAAwGmEHAAAYjbADAACMRtgBAABGI+wAAACjEXYAAIDRCDsAAMBohB0AAGA0wg4AADAaYQcAABiNsAMAAIxG2AEAAEYj7AAAAKMRdgAAgNEIOwAAwGiEHQAAYDS/hp1FixYpKSlJNptNNptNDodDa9eudW+//fbbFRQU5LE88sgjHscoKSlRRkaGIiMjFRsbqyeffFK1tbVXuhUAABCgQv355B06dNCcOXPUrVs3WZalFStWaPjw4frqq690/fXXS5ImTJigWbNmufeJjIx0/1xXV6eMjAzZ7XZt27ZNZWVlGjdunMLCwvTcc89d8X4AAEDg8WvYGTZsmMfjP/zhD1q0aJG2b9/uDjuRkZGy2+0N7v/JJ59o37592rhxo+Li4tS3b1/Nnj1b06ZN07PPPqvw8PBG7wEAAAS2gJmzU1dXp1WrVunkyZNyOBzu9W+99ZbatGmjXr16KScnRz/++KN7W35+vnr37q24uDj3uvT0dLlcLu3du/ecz1VdXS2Xy+WxAAAAM/n1zo4k7dmzRw6HQ6dOnVLLli313nvvKTExUZL0wAMPKCEhQe3bt9fu3bs1bdo0FRUV6d1335UkOZ1Oj6Ajyf3Y6XSe8zlzc3M1c+bMRuoIAAAEEr+Hne7du6uwsFCVlZX6r//6L2VmZmrr1q1KTEzUxIkT3eN69+6tdu3aKTU1VcXFxerSpYvXz5mTk6Ps7Gz3Y5fLpfj4+MvqAwAABCa/v40VHh6url27Kjk5Wbm5uerTp49efvnlBsempKRIkg4cOCBJstvtKi8v9xhz5vG55vlIUkREhPsTYGcWAABgJr+HnV+qr69XdXV1g9sKCwslSe3atZMkORwO7dmzRxUVFe4xGzZskM1mc78VBgAArm5+fRsrJydHQ4YMUceOHXXixAmtXLlSW7Zs0fr161VcXKyVK1dq6NChat26tXbv3q2pU6fqtttuU1JSkiRp0KBBSkxM1NixYzV37lw5nU5Nnz5dWVlZioiI8GdrAAAgQPg17FRUVGjcuHEqKytTVFSUkpKStH79et111106dOiQNm7cqJdeekknT55UfHy8Ro4cqenTp7v3DwkJ0Zo1azRp0iQ5HA61aNFCmZmZHt/LAwAArm5+DTtLliw557b4+Hht3br1gsdISEjQxx9/7MuyAACAQQJuzg4AAIAvEXYAAIDRCDsAAMBohB0AAGA0wg4AADAaYQcAABiNsAMAAIxG2AEAAEYj7AAAAKMRdgAAgNEIOwAAwGiEHQAAYDTCDgAAMBphBwAAGI2wAwAAjEbYAQAARiPsAAAAoxF2AACA0Qg7AADAaIQdAABgNMIOAAAwGmEHAAAYjbADAACMRtgBAABGI+wAAACjEXYAAIDRCDsAAMBohB0AAGA0wg4AADAaYQcAABiNsAMAAIxG2AEAAEYj7AAAAKMRdgAAgNH8GnYWLVqkpKQk2Ww22Ww2ORwOrV271r391KlTysrKUuvWrdWyZUuNHDlS5eXlHscoKSlRRkaGIiMjFRsbqyeffFK1tbVXuhUAABCg/Bp2OnTooDlz5qigoEBffPGF7rzzTg0fPlx79+6VJE2dOlUffvihVq9era1bt6q0tFQjRoxw719XV6eMjAydPn1a27Zt04oVK7R8+XLNmDHDXy0BAIAAE2RZluXvIn4uJiZG8+bN03333ae2bdtq5cqVuu+++yRJ3377rXr27Kn8/HwNGDBAa9eu1a9+9SuVlpYqLi5OkrR48WJNmzZNR44cUXh4+EU9p8vlUlRUlCorK2Wz2RqtN+DnOv32I3+X4JXv52T4u4SrQlP9/UDj4xr8Pxf7+h0wc3bq6uq0atUqnTx5Ug6HQwUFBaqpqVFaWpp7TI8ePdSxY0fl5+dLkvLz89W7d2930JGk9PR0uVwu992hhlRXV8vlcnksAADATH4PO3v27FHLli0VERGhRx55RO+9954SExPldDoVHh6u6Ohoj/FxcXFyOp2SJKfT6RF0zmw/s+1ccnNzFRUV5V7i4+N92xQAAAgYfg873bt3V2FhoXbs2KFJkyYpMzNT+/bta9TnzMnJUWVlpXs5dOhQoz4fAADwn1B/FxAeHq6uXbtKkpKTk7Vr1y69/PLLuv/++3X69GkdP37c4+5OeXm57Ha7JMlut2vnzp0exzvzaa0zYxoSERGhiIgIH3cCAAACkd/v7PxSfX29qqurlZycrLCwMOXl5bm3FRUVqaSkRA6HQ5LkcDi0Z88eVVRUuMds2LBBNptNiYmJV7x2AAAQePx6ZycnJ0dDhgxRx44ddeLECa1cuVJbtmzR+vXrFRUVpfHjxys7O1sxMTGy2Wx69NFH5XA4NGDAAEnSoEGDlJiYqLFjx2ru3LlyOp2aPn26srKyuHMDAAAk+TnsVFRUaNy4cSorK1NUVJSSkpK0fv163XXXXZKkBQsWKDg4WCNHjlR1dbXS09P12muvufcPCQnRmjVrNGnSJDkcDrVo0UKZmZmaNWuWv1oCAAABJuC+Z8cf+J4d+ENT/R4VvuPjymiqvx9ofFyD/+diX7/9PkEZQNPSFF+EeXEArm4BN0EZAADAlwg7AADAaIQdAABgNMIOAAAwGmEHAAAYjbADAACMRtgBAABGI+wAAACjEXYAAIDRCDsAAMBohB0AAGA0wg4AADAaYQcAABiNsAMAAIxG2AEAAEYj7AAAAKMRdgAAgNEIOwAAwGiEHQAAYDTCDgAAMBphBwAAGI2wAwAAjEbYAQAARiPsAAAAoxF2AACA0Qg7AADAaIQdAABgNMIOAAAwGmEHAAAYjbADAACMRtgBAABGI+wAAACjEXYAAIDR/Bp2cnNzdeONN6pVq1aKjY3VPffco6KiIo8xt99+u4KCgjyWRx55xGNMSUmJMjIyFBkZqdjYWD355JOqra29kq0AAIAAFerPJ9+6dauysrJ04403qra2Vk8//bQGDRqkffv2qUWLFu5xEyZM0KxZs9yPIyMj3T/X1dUpIyNDdrtd27ZtU1lZmcaNG6ewsDA999xzV7QfAAAQePwadtatW+fxePny5YqNjVVBQYFuu+029/rIyEjZ7fYGj/HJJ59o37592rhxo+Li4tS3b1/Nnj1b06ZN07PPPqvw8PBG7QEAAAS2gJqzU1lZKUmKiYnxWP/WW2+pTZs26tWrl3JycvTjjz+6t+Xn56t3796Ki4tzr0tPT5fL5dLevXsbfJ7q6mq5XC6PBQAAmMmvd3Z+rr6+Xo899phuueUW9erVy73+gQceUEJCgtq3b6/du3dr2rRpKioq0rvvvitJcjqdHkFHkvux0+ls8Llyc3M1c+bMRuoEAAAEkoAJO1lZWfrmm2/0+eefe6yfOHGi++fevXurXbt2Sk1NVXFxsbp06eLVc+Xk5Cg7O9v92OVyKT4+3rvCAQBAQAuIt7EmT56sNWvWaPPmzerQocN5x6akpEiSDhw4IEmy2+0qLy/3GHPm8bnm+URERMhms3ksAADATH4NO5ZlafLkyXrvvfe0adMmde7c+YL7FBYWSpLatWsnSXI4HNqzZ48qKircYzZs2CCbzabExMRGqRsAADQdfn0bKysrSytXrtQHH3ygVq1auefYREVFqXnz5iouLtbKlSs1dOhQtW7dWrt379bUqVN12223KSkpSZI0aNAgJSYmauzYsZo7d66cTqemT5+urKwsRURE+LM9AAAQAPx6Z2fRokWqrKzU7bffrnbt2rmXd955R5IUHh6ujRs3atCgQerRo4cef/xxjRw5Uh9++KH7GCEhIVqzZo1CQkLkcDj04IMPaty4cR7fywMAAK5efr2zY1nWebfHx8dr69atFzxOQkKCPv74Y1+VBQAADBIQE5QBAAAaC2EHAAAYjbADAACM5lXY+etf/+rrOgAAABqFV2Gna9euuuOOO/Tmm2/q1KlTvq4JAADAZ7wKO19++aWSkpKUnZ0tu92uX//619q5c6evawMAALhsXoWdvn376uWXX1ZpaamWLl2qsrIyDRw4UL169dL8+fN15MgRX9cJAADglcuaoBwaGqoRI0Zo9erVev7553XgwAE98cQTio+P17hx41RWVuarOgEAALxyWWHniy++0L/927+pXbt2mj9/vp544gkVFxdrw4YNKi0t1fDhw31VJwAAgFe8+gbl+fPna9myZSoqKtLQoUP1xhtvaOjQoQoO/ik7de7cWcuXL1enTp18WSsAAMAl8yrsLFq0SP/6r/+qhx56yP3Xx38pNjZWS5YsuaziAAAALpdXYWf//v0XHBMeHq7MzExvDg8AAOAzXs3ZWbZsmVavXn3W+tWrV2vFihWXXRQAAICveBV2cnNz1aZNm7PWx8bG6rnnnrvsogAAAHzFq7BTUlKizp07n7U+ISFBJSUll10UAACAr3gVdmJjY7V79+6z1n/99ddq3br1ZRcFAADgK16FndGjR+s3v/mNNm/erLq6OtXV1WnTpk2aMmWKRo0a5esaAQAAvObVp7Fmz56t77//XqmpqQoN/ekQ9fX1GjduHHN2AABAQPEq7ISHh+udd97R7Nmz9fXXX6t58+bq3bu3EhISfF0fAADAZfEq7Jxx3XXX6brrrvNVLQAAAD7nVdipq6vT8uXLlZeXp4qKCtXX13ts37Rpk0+KAwAAuFxehZ0pU6Zo+fLlysjIUK9evRQUFOTrugAAAHzCq7CzatUq/elPf9LQoUN9XQ8AAIBPefXR8/DwcHXt2tXXtQAAAPicV2Hn8ccf18svvyzLsnxdDwAAgE959TbW559/rs2bN2vt2rW6/vrrFRYW5rH93Xff9UlxAAAAl8ursBMdHa17773X17UAAAD4nFdhZ9myZb6uAwAAoFF4NWdHkmpra7Vx40a9/vrrOnHihCSptLRUVVVVPisOAADgcnl1Z+eHH37Q4MGDVVJSourqat11111q1aqVnn/+eVVXV2vx4sW+rhMAAMArXt3ZmTJlivr3769//OMfat68uXv9vffeq7y8PJ8VBwAAcLm8urPz2Wefadu2bQoPD/dY36lTJ/3tb3/zSWEAAAC+4NWdnfr6etXV1Z21/vDhw2rVqtVlFwUAAOArXoWdQYMG6aWXXnI/DgoKUlVVlZ555hn+hAQAAAgoXr2N9eKLLyo9PV2JiYk6deqUHnjgAe3fv19t2rTR22+/7esaAQAAvObVnZ0OHTro66+/1tNPP62pU6eqX79+mjNnjr766ivFxsZe9HFyc3N14403qlWrVoqNjdU999yjoqIijzGnTp1SVlaWWrdurZYtW2rkyJEqLy/3GFNSUqKMjAxFRkYqNjZWTz75pGpra71pDQAAGMarOzuSFBoaqgcffPCynnzr1q3KysrSjTfeqNraWj399NMaNGiQ9u3bpxYtWkiSpk6dqo8++kirV69WVFSUJk+erBEjRujPf/6zJKmurk4ZGRmy2+3atm2bysrKNG7cOIWFhem55567rPoAAEDTF2R58dc833jjjfNuHzdunFfFHDlyRLGxsdq6datuu+02VVZWqm3btlq5cqXuu+8+SdK3336rnj17Kj8/XwMGDNDatWv1q1/9SqWlpYqLi5MkLV68WNOmTdORI0fO+sRYQ1wul6KiolRZWSmbzeZV7cCl6vTbj/xdwlXj+zkZ/i7hkvH7gXNpir/PjeViX7+9urMzZcoUj8c1NTX68ccfFR4ersjISK/DTmVlpSQpJiZGklRQUKCamhqlpaW5x/To0UMdO3Z0h538/Hz17t3bHXQkKT09XZMmTdLevXvVr1+/s56nurpa1dXV7scul8uregEAQODzas7OP/7xD4+lqqpKRUVFGjhwoNcTlOvr6/XYY4/plltuUa9evSRJTqdT4eHhio6O9hgbFxcnp9PpHvPzoHNm+5ltDcnNzVVUVJR7iY+P96pmAAAQ+Lz+21i/1K1bN82ZM+esuz4XKysrS998841WrVrlq5LOKScnR5WVle7l0KFDjf6cAADAP7yeoNzgwUJDVVpaesn7TZ48WWvWrNGnn36qDh06uNfb7XadPn1ax48f97i7U15eLrvd7h6zc+dOj+Od+bTWmTG/FBERoYiIiEuuEwAAND1ehZ3/+Z//8XhsWZbKysr07//+77rlllsu+jiWZenRRx/Ve++9py1btqhz584e25OTkxUWFqa8vDyNHDlSklRUVKSSkhI5HA5JksPh0B/+8AdVVFS4P/a+YcMG2Ww2JSYmetMeAAAwiFdh55577vF4HBQUpLZt2+rOO+/Uiy++eNHHycrK0sqVK/XBBx+oVatW7jk2UVFRat68uaKiojR+/HhlZ2crJiZGNptNjz76qBwOhwYMGCDpp29zTkxM1NixYzV37lw5nU5Nnz5dWVlZ3L0BABinKX5Sz9+fIPMq7NTX1/vkyRctWiRJuv322z3WL1u2TA899JAkacGCBQoODtbIkSNVXV2t9PR0vfbaa+6xISEhWrNmjSZNmiSHw6EWLVooMzNTs2bN8kmNAACgafPpnJ1LdTFf8dOsWTMtXLhQCxcuPOeYhIQEffzxx74sDQAAGMKrsJOdnX3RY+fPn+/NUwCXpCne1gUAXBlehZ2vvvpKX331lWpqatS9e3dJ0nfffaeQkBDdcMMN7nFBQUG+qRIAAMBLXoWdYcOGqVWrVlqxYoWuueYaST990eDDDz+sW2+9VY8//rhPiwQAAPCWV18q+OKLLyo3N9cddCTpmmuu0e9///tL+jQWAABAY/Mq7LhcLh05cuSs9UeOHNGJEycuuygAAABf8Srs3HvvvXr44Yf17rvv6vDhwzp8+LD++7//W+PHj9eIESN8XSMAAIDXvJqzs3jxYj3xxBN64IEHVFNT89OBQkM1fvx4zZs3z6cFAgAAXA6vwk5kZKRee+01zZs3T8XFxZKkLl26qEWLFj4tDgAA4HJd1l89LysrU1lZmbp166YWLVpc1JcEAgAAXElehZ2jR48qNTVV1113nYYOHaqysjJJ0vjx4/nYOQAACChehZ2pU6cqLCxMJSUlioyMdK+///77tW7dOp8VBwAAcLm8mrPzySefaP369erQoYPH+m7duumHH37wSWEAAAC+4NWdnZMnT3rc0Tnj2LFjioiIuOyiAAAAfMWrsHPrrbfqjTfecD8OCgpSfX295s6dqzvuuMNnxQEAAFwur97Gmjt3rlJTU/XFF1/o9OnTeuqpp7R3714dO3ZMf/7zn31dIwAAgNe8urPTq1cvfffddxo4cKCGDx+ukydPasSIEfrqq6/UpUsXX9cIAADgtUu+s1NTU6PBgwdr8eLF+n//7/81Rk0AAAA+c8l3dsLCwrR79+7GqAUAAMDnvHob68EHH9SSJUt8XQsAAIDPeTVBuba2VkuXLtXGjRuVnJx81t/Emj9/vk+KAwAAuFyXFHb++te/qlOnTvrmm290ww03SJK+++47jzFBQUG+qw4AAOAyXVLY6datm8rKyrR582ZJP/15iFdeeUVxcXGNUhwAAMDluqQ5O7/8q+Zr167VyZMnfVoQAACAL3k1QfmMX4YfAACAQHNJYScoKOisOTnM0QEAAIHskubsWJalhx56yP3HPk+dOqVHHnnkrE9jvfvuu76rEAAA4DJcUtjJzMz0ePzggw/6tBgAAABfu6Sws2zZssaqAwAAoFFc1gRlAACAQEfYAQAARiPsAAAAoxF2AACA0Qg7AADAaIQdAABgNL+GnU8//VTDhg1T+/btFRQUpPfff99j+0MPPeT+1uYzy+DBgz3GHDt2TGPGjJHNZlN0dLTGjx+vqqqqK9gFAAAIZH4NOydPnlSfPn20cOHCc44ZPHiwysrK3Mvbb7/tsX3MmDHau3evNmzYoDVr1ujTTz/VxIkTG7t0AADQRFzSlwr62pAhQzRkyJDzjomIiJDdbm9w21/+8hetW7dOu3btUv/+/SVJr776qoYOHaoXXnhB7du393nNAACgaQn4OTtbtmxRbGysunfvrkmTJuno0aPubfn5+YqOjnYHHUlKS0tTcHCwduzYcc5jVldXy+VyeSwAAMBMAR12Bg8erDfeeEN5eXl6/vnntXXrVg0ZMkR1dXWSJKfTqdjYWI99QkNDFRMTI6fTec7j5ubmKioqyr3Ex8c3ah8AAMB//Po21oWMGjXK/XPv3r2VlJSkLl26aMuWLUpNTfX6uDk5OcrOznY/drlcBB4AAAwV0Hd2funaa69VmzZtdODAAUmS3W5XRUWFx5ja2lodO3bsnPN8pJ/mAdlsNo8FAACYqUmFncOHD+vo0aNq166dJMnhcOj48eMqKChwj9m0aZPq6+uVkpLirzIBAEAA8evbWFVVVe67NJJ08OBBFRYWKiYmRjExMZo5c6ZGjhwpu92u4uJiPfXUU+ratavS09MlST179tTgwYM1YcIELV68WDU1NZo8ebJGjRrFJ7EAAIAkP9/Z+eKLL9SvXz/169dPkpSdna1+/fppxowZCgkJ0e7du3X33Xfruuuu0/jx45WcnKzPPvtMERER7mO89dZb6tGjh1JTUzV06FANHDhQ//Ef/+GvlgAAQIDx652d22+/XZZlnXP7+vXrL3iMmJgYrVy50pdlAQAAgzSpOTsAAACXirADAACMRtgBAABGI+wAAACjEXYAAIDRCDsAAMBohB0AAGA0wg4AADAaYQcAABiNsAMAAIxG2AEAAEYj7AAAAKMRdgAAgNEIOwAAwGiEHQAAYDTCDgAAMBphBwAAGI2wAwAAjEbYAQAARiPsAAAAoxF2AACA0Qg7AADAaIQdAABgNMIOAAAwGmEHAAAYjbADAACMRtgBAABGI+wAAACjEXYAAIDRCDsAAMBohB0AAGA0wg4AADAaYQcAABiNsAMAAIzm17Dz6aefatiwYWrfvr2CgoL0/vvve2y3LEszZsxQu3bt1Lx5c6WlpWn//v0eY44dO6YxY8bIZrMpOjpa48ePV1VV1RXsAgAABDK/hp2TJ0+qT58+WrhwYYPb586dq1deeUWLFy/Wjh071KJFC6Wnp+vUqVPuMWPGjNHevXu1YcMGrVmzRp9++qkmTpx4pVoAAAABLtSfTz5kyBANGTKkwW2WZemll17S9OnTNXz4cEnSG2+8obi4OL3//vsaNWqU/vKXv2jdunXatWuX+vfvL0l69dVXNXToUL3wwgtq3779FesFAAAEpoCds3Pw4EE5nU6lpaW510VFRSklJUX5+fmSpPz8fEVHR7uDjiSlpaUpODhYO3bsuOI1AwCAwOPXOzvn43Q6JUlxcXEe6+Pi4tzbnE6nYmNjPbaHhoYqJibGPaYh1dXVqq6udj92uVy+KhsAAASYgA07jSk3N1czZ868Is/V6bcfXZHn8aXv52T4uwQAAHwmYMOO3W6XJJWXl6tdu3bu9eXl5erbt697TEVFhcd+tbW1OnbsmHv/huTk5Cg7O9v92OVyKT4+3ofVAwgkTfF/OgD4TsDO2encubPsdrvy8vLc61wul3bs2CGHwyFJcjgcOn78uAoKCtxjNm3apPr6eqWkpJzz2BEREbLZbB4LAAAwk1/v7FRVVenAgQPuxwcPHlRhYaFiYmLUsWNHPfbYY/r973+vbt26qXPnzvrd736n9u3b65577pEk9ezZU4MHD9aECRO0ePFi1dTUaPLkyRo1ahSfxAIAAJL8HHa++OIL3XHHHe7HZ95ayszM1PLly/XUU0/p5MmTmjhxoo4fP66BAwdq3bp1atasmXuft956S5MnT1ZqaqqCg4M1cuRIvfLKK1e8FwAAEJiCLMuy/F2Ev7lcLkVFRamystLnb2k1xbkCTXGCclP8dwaAq0Vjva5c7Ot3wM7ZAQAA8AXCDgAAMBphBwAAGI2wAwAAjEbYAQAARiPsAAAAoxF2AACA0Qg7AADAaIQdAABgNMIOAAAwGmEHAAAYjbADAACMRtgBAABGI+wAAACjEXYAAIDRCDsAAMBohB0AAGA0wg4AADAaYQcAABiNsAMAAIxG2AEAAEYj7AAAAKMRdgAAgNEIOwAAwGiEHQAAYDTCDgAAMBphBwAAGI2wAwAAjEbYAQAARiPsAAAAoxF2AACA0Qg7AADAaIQdAABgNMIOAAAwGmEHAAAYLaDDzrPPPqugoCCPpUePHu7tp06dUlZWllq3bq2WLVtq5MiRKi8v92PFAAAg0AR02JGk66+/XmVlZe7l888/d2+bOnWqPvzwQ61evVpbt25VaWmpRowY4cdqAQBAoAn1dwEXEhoaKrvdftb6yspKLVmyRCtXrtSdd94pSVq2bJl69uyp7du3a8CAAVe6VAAAEIAC/s7O/v371b59e1177bUaM2aMSkpKJEkFBQWqqalRWlqae2yPHj3UsWNH5efnn/eY1dXVcrlcHgsAADBTQIedlJQULV++XOvWrdOiRYt08OBB3XrrrTpx4oScTqfCw8MVHR3tsU9cXJycTud5j5ubm6uoqCj3Eh8f34hdAAAAfwrot7GGDBni/jkpKUkpKSlKSEjQn/70JzVv3tzr4+bk5Cg7O9v92OVyEXgAADBUQN/Z+aXo6Ghdd911OnDggOx2u06fPq3jx497jCkvL29wjs/PRUREyGazeSwAAMBMTSrsVFVVqbi4WO3atVNycrLCwsKUl5fn3l5UVKSSkhI5HA4/VgkAAAJJQL+N9cQTT2jYsGFKSEhQaWmpnnnmGYWEhGj06NGKiorS+PHjlZ2drZiYGNlsNj366KNyOBx8EgsAALgFdNg5fPiwRo8eraNHj6pt27YaOHCgtm/frrZt20qSFixYoODgYI0cOVLV1dVKT0/Xa6+95ueqAQBAIAnosLNq1arzbm/WrJkWLlyohQsXXqGKAABAU9Ok5uwAAABcKsIOAAAwGmEHAAAYjbADAACMRtgBAABGI+wAAACjEXYAAIDRCDsAAMBohB0AAGA0wg4AADAaYQcAABiNsAMAAIxG2AEAAEYj7AAAAKMRdgAAgNEIOwAAwGiEHQAAYDTCDgAAMBphBwAAGI2wAwAAjEbYAQAARiPsAAAAoxF2AACA0Qg7AADAaIQdAABgNMIOAAAwGmEHAAAYjbADAACMRtgBAABGI+wAAACjEXYAAIDRCDsAAMBohB0AAGA0wg4AADCaMWFn4cKF6tSpk5o1a6aUlBTt3LnT3yUBAIAAYETYeeedd5Sdna1nnnlGX375pfr06aP09HRVVFT4uzQAAOBnRoSd+fPna8KECXr44YeVmJioxYsXKzIyUkuXLvV3aQAAwM9C/V3A5Tp9+rQKCgqUk5PjXhccHKy0tDTl5+c3uE91dbWqq6vdjysrKyVJLpfL5/XVV//o82M2tsb4d2hsTfHfGQCuFo31unLmuJZlnXdckw87f//731VXV6e4uDiP9XFxcfr2228b3Cc3N1czZ848a318fHyj1NjURL3k7woAACZp7NeVEydOKCoq6pzbm3zY8UZOTo6ys7Pdj+vr63Xs2DG1bt1aQUFBV6wOl8ul+Ph4HTp0SDab7Yo9r7/RN31fDeibvq8G/u7bsiydOHFC7du3P++4Jh922rRpo5CQEJWXl3usLy8vl91ub3CfiIgIRUREeKyLjo5urBIvyGazXVUXxxn0fXWh76sLfV9d/Nn3+e7onNHkJyiHh4crOTlZeXl57nX19fXKy8uTw+HwY2UAACAQNPk7O5KUnZ2tzMxM9e/fXzfddJNeeuklnTx5Ug8//LC/SwMAAH5mRNi5//77deTIEc2YMUNOp1N9+/bVunXrzpq0HGgiIiL0zDPPnPWWmunom76vBvRN31eDptJ3kHWhz2sBAAA0YU1+zg4AAMD5EHYAAIDRCDsAAMBohB0AAGA0ws5lWLhwoTp16qRmzZopJSVFO3fuvKj9Vq1apaCgIN1zzz0e6y3L0owZM9SuXTs1b95caWlp2r9/v8eYY8eOacyYMbLZbIqOjtb48eNVVVXlq5Yuii/7rqmp0bRp09S7d2+1aNFC7du317hx41RaWuqxb6dOnRQUFOSxzJkzx5dtXZCvz/dDDz10Vk+DBw/2GGPa+ZZ0Vs9nlnnz5rnHNLXzvXz58rPqbdasmccYE6/vC/Vt6vV9MefbxOv7YvoO2OvbgldWrVplhYeHW0uXLrX27t1rTZgwwYqOjrbKy8vPu9/Bgwetf/qnf7JuvfVWa/jw4R7b5syZY0VFRVnvv/++9fXXX1t333231blzZ+t///d/3WMGDx5s9enTx9q+fbv12WefWV27drVGjx7dGC02yNd9Hz9+3EpLS7Peeecd69tvv7Xy8/Otm266yUpOTvbYPyEhwZo1a5ZVVlbmXqqqqhqjxQY1xvnOzMy0Bg8e7NHTsWPHPMaYdr4ty/Lot6yszFq6dKkVFBRkFRcXu8c0tfO9bNkyy2azedTrdDo9xph4fV+ob1Ov74s53yZe3xfTd6Be34QdL910001WVlaW+3FdXZ3Vvn17Kzc395z71NbWWjfffLP1n//5n1ZmZqbHi0B9fb1lt9utefPmudcdP37cioiIsN5++23Lsixr3759liRr165d7jFr1661goKCrL/97W8+7O7cfN13Q3bu3GlJsn744Qf3uoSEBGvBggWXW77XGqPvC/1bXC3ne/jw4dadd97psa6pne9ly5ZZUVFR5zyeqdf3hfpuiAnX98X0beL17c35DpTrm7exvHD69GkVFBQoLS3NvS44OFhpaWnKz88/536zZs1SbGysxo8ff9a2gwcPyul0ehwzKipKKSkp7mPm5+crOjpa/fv3d49JS0tTcHCwduzY4YvWzqsx+m5IZWWlgoKCzvp7ZXPmzFHr1q3Vr18/zZs3T7W1tV71cakas+8tW7YoNjZW3bt316RJk3T06FH3tqvhfJeXl+ujjz5qcGxTO99VVVVKSEhQfHy8hg8frr1797q3mXx9n6/vhphyfV9M3yZe35dyvgPp+jbiG5SvtL///e+qq6s76xua4+Li9O233za4z+eff64lS5aosLCwwe1Op9N9jF8e88w2p9Op2NhYj+2hoaGKiYlxj2lMjdH3L506dUrTpk3T6NGjPf6o3G9+8xvdcMMNiomJ0bZt25STk6OysjLNnz/f634uVmP1PXjwYI0YMUKdO3dWcXGxnn76aQ0ZMkT5+fkKCQm5Ks73ihUr1KpVK40YMcJjfVM73927d9fSpUuVlJSkyspKvfDCC7r55pu1d+9edejQwdjr+0J9/5Ip1/fF9G3i9X2p5zuQrm/CzhVw4sQJjR07Vn/84x/Vpk0bf5dzxVxq3zU1NfqXf/kXWZalRYsWeWzLzs52/5yUlKTw8HD9+te/Vm5ubsB9TfnF9j1q1Cj3z71791ZSUpK6dOmiLVu2KDU19UqU6lPe/J4vXbpUY8aMOWuSY1M635LkcDg8/vDwzTffrJ49e+r111/X7Nmz/VhZ47qUvk25vqWL69u061u69N/zQLq+CTteaNOmjUJCQlReXu6xvry8XHa7/azxxcXF+v777zVs2DD3uvr6ekk/JfmioiL3fuXl5WrXrp3HMfv27StJstvtqqio8Dh2bW2tjh071uDz+lpj9N2lSxdJ//cfwh9++EGbNm3y+L++hqSkpKi2tlbff/+9unfvfrmtnVdj9v1z1157rdq0aaMDBw4oNTXV6PMtSZ999pmKior0zjvvXLCWQD7fDQkLC1O/fv104MABSTLy+m7IL/s+w6TruyHn6vvnmvr13ZDz9R1o1zdzdrwQHh6u5ORk5eXludfV19crLy/PI/We0aNHD+3Zs0eFhYXu5e6779Ydd9yhwsJCxcfHq3PnzrLb7R7HdLlc2rFjh/uYDodDx48fV0FBgXvMpk2bVF9fr5SUlEbs+CeN0bf0f/8h3L9/vzZu3KjWrVtfsJbCwkIFBwefdRu4MTRW3790+PBhHT161P1iaOr5PmPJkiVKTk5Wnz59LlhLIJ/vhtTV1WnPnj3uc2ni9d2QX/YtmXd9N6Shvn+pqV/fDTlf3wF3fV/R6dAGWbVqlRUREWEtX77c2rdvnzVx4kQrOjra/TG8sWPHWr/97W/PuX9DM/XnzJljRUdHWx988IG1e/dua/jw4Q1+NLVfv37Wjh07rM8//9zq1q3bFf+ooi/7Pn36tHX33XdbHTp0sAoLCz0+ilhdXW1ZlmVt27bNWrBggVVYWGgVFxdbb775ptW2bVtr3Lhxjdrrz/m67xMnTlhPPPGElZ+fbx08eNDauHGjdcMNN1jdunWzTp065R5n2vk+o7Ky0oqMjLQWLVp01rameL5nzpxprV+/3iouLrYKCgqsUaNGWc2aNbP27t3rHmPi9X2hvk29vi/Ut6nX98X8nltWYF7fhJ3L8Oqrr1odO3a0wsPDrZtuusnavn27e9s///M/W5mZmefct6EXgfr6eut3v/udFRcXZ0VERFipqalWUVGRx5ijR49ao0ePtlq2bGnZbDbr4Ycftk6cOOHLti7Il30fPHjQktTgsnnzZsuyLKugoMBKSUmxoqKirGbNmlk9e/a0nnvuOY//aFwJvuz7xx9/tAYNGmS1bdvWCgsLsxISEqwJEyac9Z0Vpp3vM15//XWrefPm1vHjx8/a1hTP92OPPeYeGxcXZw0dOtT68ssvPY5n4vV9ob5Nvb4v1Lep1/fF/J5bVmBe30GWZVmNd98IAADAv5izAwAAjEbYAQAARiPsAAAAoxF2AACA0Qg7AADAaIQdAABgNMIOAAAwGmEHAAAYjbADAACMRtgBAABGI+wAAACjEXYAAIDR/j/Io6f4VAtCjQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "aaa.groupby('tar_id')['Score'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0339ae1f-44de-4485-8246-d92b4d979ed4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tar_id\n",
       "4a1a84eb-24c9-4836-8361-94a5f009b328    25\n",
       "fdfd78d3-2667-4c6f-97f6-6f8aa2404bd2    25\n",
       "f1b2463e-895f-4445-9742-e676e862c505    25\n",
       "c7afa039-9c7b-4e5d-b1a0-aa63881cebb7    25\n",
       "095e704d-2219-4b96-9b98-de62be23e773    25\n",
       "                                        ..\n",
       "e03c8acc-2281-4bdd-8038-1ff415b50e9e    17\n",
       "f960b043-f982-42a8-a085-508ef4352669    16\n",
       "0d29d2df-1e16-4546-975a-e7467ad7ff80    15\n",
       "8328e6c5-6494-4bf5-b5f5-a0339a9ac2cc    13\n",
       "9c87949f-0398-4a6a-a5e0-02e309c088e1     6\n",
       "Name: count, Length: 100, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aaa['tar_id'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "90578b7c-7095-4b45-b88f-26b5bc089c89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2418"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(aaa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "78c25d55-66aa-4a03-a445-131167f103a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tar_id                  970ffa60-ff80-4694-b334-3d3a12213ffe\n",
       "Model                                    asaf_kagan_20250226\n",
       "RouterLabel    tensor(0, device='cuda:0', dtype=torch.int32)\n",
       "Score                        tensor(0.5522, device='cuda:0')\n",
       "we                               tensor(7., device='cuda:0')\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aaa.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7bd6c917-e76e-4e8b-9852-d959c21e06cf",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiRouterModelV0(\n",
       "  (module_dict): ModuleDict(\n",
       "    (asaf_kagan_20250226): Sequential(\n",
       "      (0): Sequential(\n",
       "        (0): Unflatten(dim=1, unflattened_size=[1, 512])\n",
       "        (1): Conv2d(1, 1, kernel_size=(5, 5), stride=(2, 2))\n",
       "        (2): Flatten(start_dim=1, end_dim=2)\n",
       "        (3): Mish()\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): Conv1d(254, 256, kernel_size=(5,), stride=(2,))\n",
       "        (1): Mish()\n",
       "        (2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (3): Transpose()\n",
       "        (4): SelfAttention(\n",
       "          (attention_layer): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (5): Transpose()\n",
       "      )\n",
       "      (2): Sequential(\n",
       "        (0): Conv1d(256, 64, kernel_size=(5,), stride=(2,))\n",
       "        (1): Mish()\n",
       "        (2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (3): Transpose()\n",
       "        (4): SelfAttention(\n",
       "          (attention_layer): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (5): Transpose()\n",
       "      )\n",
       "      (3): Sequential(\n",
       "        (0): AdaptiveAvgPool1d(output_size=3)\n",
       "        (1): Conv1d(64, 32, kernel_size=(3,), stride=(1,))\n",
       "        (2): Flatten(start_dim=-2, end_dim=-1)\n",
       "        (3): Mish()\n",
       "        (4): Linear(in_features=32, out_features=16, bias=True)\n",
       "        (5): Linear(in_features=16, out_features=1, bias=True)\n",
       "        (6): Flatten(start_dim=0, end_dim=-1)\n",
       "      )\n",
       "    )\n",
       "    (avi_barliya_20250312): Sequential(\n",
       "      (0): Sequential(\n",
       "        (0): Unflatten(dim=1, unflattened_size=[1, 512])\n",
       "        (1): Conv2d(1, 1, kernel_size=(5, 5), stride=(2, 2))\n",
       "        (2): Flatten(start_dim=1, end_dim=2)\n",
       "        (3): Mish()\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): Conv1d(254, 256, kernel_size=(5,), stride=(2,))\n",
       "        (1): Mish()\n",
       "        (2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (3): Transpose()\n",
       "        (4): SelfAttention(\n",
       "          (attention_layer): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (5): Transpose()\n",
       "      )\n",
       "      (2): Sequential(\n",
       "        (0): Conv1d(256, 64, kernel_size=(5,), stride=(2,))\n",
       "        (1): Mish()\n",
       "        (2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (3): Transpose()\n",
       "        (4): SelfAttention(\n",
       "          (attention_layer): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (5): Transpose()\n",
       "      )\n",
       "      (3): Sequential(\n",
       "        (0): AdaptiveAvgPool1d(output_size=3)\n",
       "        (1): Conv1d(64, 32, kernel_size=(3,), stride=(1,))\n",
       "        (2): Flatten(start_dim=-2, end_dim=-1)\n",
       "        (3): Mish()\n",
       "        (4): Linear(in_features=32, out_features=16, bias=True)\n",
       "        (5): Linear(in_features=16, out_features=1, bias=True)\n",
       "        (6): Flatten(start_dim=0, end_dim=-1)\n",
       "      )\n",
       "    )\n",
       "    (aviad_maizels_20250312): Sequential(\n",
       "      (0): Sequential(\n",
       "        (0): Unflatten(dim=1, unflattened_size=[1, 512])\n",
       "        (1): Conv2d(1, 1, kernel_size=(5, 5), stride=(2, 2))\n",
       "        (2): Flatten(start_dim=1, end_dim=2)\n",
       "        (3): Mish()\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): Conv1d(254, 256, kernel_size=(5,), stride=(2,))\n",
       "        (1): Mish()\n",
       "        (2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (3): Transpose()\n",
       "        (4): SelfAttention(\n",
       "          (attention_layer): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (5): Transpose()\n",
       "      )\n",
       "      (2): Sequential(\n",
       "        (0): Conv1d(256, 64, kernel_size=(5,), stride=(2,))\n",
       "        (1): Mish()\n",
       "        (2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (3): Transpose()\n",
       "        (4): SelfAttention(\n",
       "          (attention_layer): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (5): Transpose()\n",
       "      )\n",
       "      (3): Sequential(\n",
       "        (0): AdaptiveAvgPool1d(output_size=3)\n",
       "        (1): Conv1d(64, 32, kernel_size=(3,), stride=(1,))\n",
       "        (2): Flatten(start_dim=-2, end_dim=-1)\n",
       "        (3): Mish()\n",
       "        (4): Linear(in_features=32, out_features=16, bias=True)\n",
       "        (5): Linear(in_features=16, out_features=1, bias=True)\n",
       "        (6): Flatten(start_dim=0, end_dim=-1)\n",
       "      )\n",
       "    )\n",
       "    (bamboo_sleeper_20250312): Sequential(\n",
       "      (0): Sequential(\n",
       "        (0): Unflatten(dim=1, unflattened_size=[1, 512])\n",
       "        (1): Conv2d(1, 1, kernel_size=(5, 5), stride=(2, 2))\n",
       "        (2): Flatten(start_dim=1, end_dim=2)\n",
       "        (3): Mish()\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): Conv1d(254, 256, kernel_size=(5,), stride=(2,))\n",
       "        (1): Mish()\n",
       "        (2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (3): Transpose()\n",
       "        (4): SelfAttention(\n",
       "          (attention_layer): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (5): Transpose()\n",
       "      )\n",
       "      (2): Sequential(\n",
       "        (0): Conv1d(256, 64, kernel_size=(5,), stride=(2,))\n",
       "        (1): Mish()\n",
       "        (2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (3): Transpose()\n",
       "        (4): SelfAttention(\n",
       "          (attention_layer): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (5): Transpose()\n",
       "      )\n",
       "      (3): Sequential(\n",
       "        (0): AdaptiveAvgPool1d(output_size=3)\n",
       "        (1): Conv1d(64, 32, kernel_size=(3,), stride=(1,))\n",
       "        (2): Flatten(start_dim=-2, end_dim=-1)\n",
       "        (3): Mish()\n",
       "        (4): Linear(in_features=32, out_features=16, bias=True)\n",
       "        (5): Linear(in_features=16, out_features=1, bias=True)\n",
       "        (6): Flatten(start_dim=0, end_dim=-1)\n",
       "      )\n",
       "    )\n",
       "    (christopher_gray_20250312): Sequential(\n",
       "      (0): Sequential(\n",
       "        (0): Unflatten(dim=1, unflattened_size=[1, 512])\n",
       "        (1): Conv2d(1, 1, kernel_size=(5, 5), stride=(2, 2))\n",
       "        (2): Flatten(start_dim=1, end_dim=2)\n",
       "        (3): Mish()\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): Conv1d(254, 256, kernel_size=(5,), stride=(2,))\n",
       "        (1): Mish()\n",
       "        (2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (3): Transpose()\n",
       "        (4): SelfAttention(\n",
       "          (attention_layer): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (5): Transpose()\n",
       "      )\n",
       "      (2): Sequential(\n",
       "        (0): Conv1d(256, 64, kernel_size=(5,), stride=(2,))\n",
       "        (1): Mish()\n",
       "        (2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (3): Transpose()\n",
       "        (4): SelfAttention(\n",
       "          (attention_layer): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (5): Transpose()\n",
       "      )\n",
       "      (3): Sequential(\n",
       "        (0): AdaptiveAvgPool1d(output_size=3)\n",
       "        (1): Conv1d(64, 32, kernel_size=(3,), stride=(1,))\n",
       "        (2): Flatten(start_dim=-2, end_dim=-1)\n",
       "        (3): Mish()\n",
       "        (4): Linear(in_features=32, out_features=16, bias=True)\n",
       "        (5): Linear(in_features=16, out_features=1, bias=True)\n",
       "        (6): Flatten(start_dim=0, end_dim=-1)\n",
       "      )\n",
       "    )\n",
       "    (daniel_asherov_20250224): Sequential(\n",
       "      (0): Sequential(\n",
       "        (0): Unflatten(dim=1, unflattened_size=[1, 512])\n",
       "        (1): Conv2d(1, 1, kernel_size=(5, 5), stride=(2, 2))\n",
       "        (2): Flatten(start_dim=1, end_dim=2)\n",
       "        (3): Mish()\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): Conv1d(254, 256, kernel_size=(5,), stride=(2,))\n",
       "        (1): Mish()\n",
       "        (2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (3): Transpose()\n",
       "        (4): SelfAttention(\n",
       "          (attention_layer): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (5): Transpose()\n",
       "      )\n",
       "      (2): Sequential(\n",
       "        (0): Conv1d(256, 64, kernel_size=(5,), stride=(2,))\n",
       "        (1): Mish()\n",
       "        (2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (3): Transpose()\n",
       "        (4): SelfAttention(\n",
       "          (attention_layer): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (5): Transpose()\n",
       "      )\n",
       "      (3): Sequential(\n",
       "        (0): AdaptiveAvgPool1d(output_size=3)\n",
       "        (1): Conv1d(64, 32, kernel_size=(3,), stride=(1,))\n",
       "        (2): Flatten(start_dim=-2, end_dim=-1)\n",
       "        (3): Mish()\n",
       "        (4): Linear(in_features=32, out_features=16, bias=True)\n",
       "        (5): Linear(in_features=16, out_features=1, bias=True)\n",
       "        (6): Flatten(start_dim=0, end_dim=-1)\n",
       "      )\n",
       "    )\n",
       "    (dennis_fuentes_20250312): Sequential(\n",
       "      (0): Sequential(\n",
       "        (0): Unflatten(dim=1, unflattened_size=[1, 512])\n",
       "        (1): Conv2d(1, 1, kernel_size=(5, 5), stride=(2, 2))\n",
       "        (2): Flatten(start_dim=1, end_dim=2)\n",
       "        (3): Mish()\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): Conv1d(254, 256, kernel_size=(5,), stride=(2,))\n",
       "        (1): Mish()\n",
       "        (2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (3): Transpose()\n",
       "        (4): SelfAttention(\n",
       "          (attention_layer): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (5): Transpose()\n",
       "      )\n",
       "      (2): Sequential(\n",
       "        (0): Conv1d(256, 64, kernel_size=(5,), stride=(2,))\n",
       "        (1): Mish()\n",
       "        (2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (3): Transpose()\n",
       "        (4): SelfAttention(\n",
       "          (attention_layer): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (5): Transpose()\n",
       "      )\n",
       "      (3): Sequential(\n",
       "        (0): AdaptiveAvgPool1d(output_size=3)\n",
       "        (1): Conv1d(64, 32, kernel_size=(3,), stride=(1,))\n",
       "        (2): Flatten(start_dim=-2, end_dim=-1)\n",
       "        (3): Mish()\n",
       "        (4): Linear(in_features=32, out_features=16, bias=True)\n",
       "        (5): Linear(in_features=16, out_features=1, bias=True)\n",
       "        (6): Flatten(start_dim=0, end_dim=-1)\n",
       "      )\n",
       "    )\n",
       "    (diver_express_20250312): Sequential(\n",
       "      (0): Sequential(\n",
       "        (0): Unflatten(dim=1, unflattened_size=[1, 512])\n",
       "        (1): Conv2d(1, 1, kernel_size=(5, 5), stride=(2, 2))\n",
       "        (2): Flatten(start_dim=1, end_dim=2)\n",
       "        (3): Mish()\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): Conv1d(254, 256, kernel_size=(5,), stride=(2,))\n",
       "        (1): Mish()\n",
       "        (2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (3): Transpose()\n",
       "        (4): SelfAttention(\n",
       "          (attention_layer): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (5): Transpose()\n",
       "      )\n",
       "      (2): Sequential(\n",
       "        (0): Conv1d(256, 64, kernel_size=(5,), stride=(2,))\n",
       "        (1): Mish()\n",
       "        (2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (3): Transpose()\n",
       "        (4): SelfAttention(\n",
       "          (attention_layer): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (5): Transpose()\n",
       "      )\n",
       "      (3): Sequential(\n",
       "        (0): AdaptiveAvgPool1d(output_size=3)\n",
       "        (1): Conv1d(64, 32, kernel_size=(3,), stride=(1,))\n",
       "        (2): Flatten(start_dim=-2, end_dim=-1)\n",
       "        (3): Mish()\n",
       "        (4): Linear(in_features=32, out_features=16, bias=True)\n",
       "        (5): Linear(in_features=16, out_features=1, bias=True)\n",
       "        (6): Flatten(start_dim=0, end_dim=-1)\n",
       "      )\n",
       "    )\n",
       "    (doron_gazit_20250225): Sequential(\n",
       "      (0): Sequential(\n",
       "        (0): Unflatten(dim=1, unflattened_size=[1, 512])\n",
       "        (1): Conv2d(1, 1, kernel_size=(5, 5), stride=(2, 2))\n",
       "        (2): Flatten(start_dim=1, end_dim=2)\n",
       "        (3): Mish()\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): Conv1d(254, 256, kernel_size=(5,), stride=(2,))\n",
       "        (1): Mish()\n",
       "        (2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (3): Transpose()\n",
       "        (4): SelfAttention(\n",
       "          (attention_layer): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (5): Transpose()\n",
       "      )\n",
       "      (2): Sequential(\n",
       "        (0): Conv1d(256, 64, kernel_size=(5,), stride=(2,))\n",
       "        (1): Mish()\n",
       "        (2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (3): Transpose()\n",
       "        (4): SelfAttention(\n",
       "          (attention_layer): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (5): Transpose()\n",
       "      )\n",
       "      (3): Sequential(\n",
       "        (0): AdaptiveAvgPool1d(output_size=3)\n",
       "        (1): Conv1d(64, 32, kernel_size=(3,), stride=(1,))\n",
       "        (2): Flatten(start_dim=-2, end_dim=-1)\n",
       "        (3): Mish()\n",
       "        (4): Linear(in_features=32, out_features=16, bias=True)\n",
       "        (5): Linear(in_features=16, out_features=1, bias=True)\n",
       "        (6): Flatten(start_dim=0, end_dim=-1)\n",
       "      )\n",
       "    )\n",
       "    (eran_roll_20250312): Sequential(\n",
       "      (0): Sequential(\n",
       "        (0): Unflatten(dim=1, unflattened_size=[1, 512])\n",
       "        (1): Conv2d(1, 1, kernel_size=(5, 5), stride=(2, 2))\n",
       "        (2): Flatten(start_dim=1, end_dim=2)\n",
       "        (3): Mish()\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): Conv1d(254, 256, kernel_size=(5,), stride=(2,))\n",
       "        (1): Mish()\n",
       "        (2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (3): Transpose()\n",
       "        (4): SelfAttention(\n",
       "          (attention_layer): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (5): Transpose()\n",
       "      )\n",
       "      (2): Sequential(\n",
       "        (0): Conv1d(256, 64, kernel_size=(5,), stride=(2,))\n",
       "        (1): Mish()\n",
       "        (2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (3): Transpose()\n",
       "        (4): SelfAttention(\n",
       "          (attention_layer): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (5): Transpose()\n",
       "      )\n",
       "      (3): Sequential(\n",
       "        (0): AdaptiveAvgPool1d(output_size=3)\n",
       "        (1): Conv1d(64, 32, kernel_size=(3,), stride=(1,))\n",
       "        (2): Flatten(start_dim=-2, end_dim=-1)\n",
       "        (3): Mish()\n",
       "        (4): Linear(in_features=32, out_features=16, bias=True)\n",
       "        (5): Linear(in_features=16, out_features=1, bias=True)\n",
       "        (6): Flatten(start_dim=0, end_dim=-1)\n",
       "      )\n",
       "    )\n",
       "    (face_body_20250312): Sequential(\n",
       "      (0): Sequential(\n",
       "        (0): Unflatten(dim=1, unflattened_size=[1, 512])\n",
       "        (1): Conv2d(1, 1, kernel_size=(5, 5), stride=(2, 2))\n",
       "        (2): Flatten(start_dim=1, end_dim=2)\n",
       "        (3): Mish()\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): Conv1d(254, 256, kernel_size=(5,), stride=(2,))\n",
       "        (1): Mish()\n",
       "        (2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (3): Transpose()\n",
       "        (4): SelfAttention(\n",
       "          (attention_layer): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (5): Transpose()\n",
       "      )\n",
       "      (2): Sequential(\n",
       "        (0): Conv1d(256, 64, kernel_size=(5,), stride=(2,))\n",
       "        (1): Mish()\n",
       "        (2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (3): Transpose()\n",
       "        (4): SelfAttention(\n",
       "          (attention_layer): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (5): Transpose()\n",
       "      )\n",
       "      (3): Sequential(\n",
       "        (0): AdaptiveAvgPool1d(output_size=3)\n",
       "        (1): Conv1d(64, 32, kernel_size=(3,), stride=(1,))\n",
       "        (2): Flatten(start_dim=-2, end_dim=-1)\n",
       "        (3): Mish()\n",
       "        (4): Linear(in_features=32, out_features=16, bias=True)\n",
       "        (5): Linear(in_features=16, out_features=1, bias=True)\n",
       "        (6): Flatten(start_dim=0, end_dim=-1)\n",
       "      )\n",
       "    )\n",
       "    (guy_maich_20250312): Sequential(\n",
       "      (0): Sequential(\n",
       "        (0): Unflatten(dim=1, unflattened_size=[1, 512])\n",
       "        (1): Conv2d(1, 1, kernel_size=(5, 5), stride=(2, 2))\n",
       "        (2): Flatten(start_dim=1, end_dim=2)\n",
       "        (3): Mish()\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): Conv1d(254, 256, kernel_size=(5,), stride=(2,))\n",
       "        (1): Mish()\n",
       "        (2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (3): Transpose()\n",
       "        (4): SelfAttention(\n",
       "          (attention_layer): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (5): Transpose()\n",
       "      )\n",
       "      (2): Sequential(\n",
       "        (0): Conv1d(256, 64, kernel_size=(5,), stride=(2,))\n",
       "        (1): Mish()\n",
       "        (2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (3): Transpose()\n",
       "        (4): SelfAttention(\n",
       "          (attention_layer): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (5): Transpose()\n",
       "      )\n",
       "      (3): Sequential(\n",
       "        (0): AdaptiveAvgPool1d(output_size=3)\n",
       "        (1): Conv1d(64, 32, kernel_size=(3,), stride=(1,))\n",
       "        (2): Flatten(start_dim=-2, end_dim=-1)\n",
       "        (3): Mish()\n",
       "        (4): Linear(in_features=32, out_features=16, bias=True)\n",
       "        (5): Linear(in_features=16, out_features=1, bias=True)\n",
       "        (6): Flatten(start_dim=0, end_dim=-1)\n",
       "      )\n",
       "    )\n",
       "    (leonid_pakman_20250312): Sequential(\n",
       "      (0): Sequential(\n",
       "        (0): Unflatten(dim=1, unflattened_size=[1, 512])\n",
       "        (1): Conv2d(1, 1, kernel_size=(5, 5), stride=(2, 2))\n",
       "        (2): Flatten(start_dim=1, end_dim=2)\n",
       "        (3): Mish()\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): Conv1d(254, 256, kernel_size=(5,), stride=(2,))\n",
       "        (1): Mish()\n",
       "        (2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (3): Transpose()\n",
       "        (4): SelfAttention(\n",
       "          (attention_layer): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (5): Transpose()\n",
       "      )\n",
       "      (2): Sequential(\n",
       "        (0): Conv1d(256, 64, kernel_size=(5,), stride=(2,))\n",
       "        (1): Mish()\n",
       "        (2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (3): Transpose()\n",
       "        (4): SelfAttention(\n",
       "          (attention_layer): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (5): Transpose()\n",
       "      )\n",
       "      (3): Sequential(\n",
       "        (0): AdaptiveAvgPool1d(output_size=3)\n",
       "        (1): Conv1d(64, 32, kernel_size=(3,), stride=(1,))\n",
       "        (2): Flatten(start_dim=-2, end_dim=-1)\n",
       "        (3): Mish()\n",
       "        (4): Linear(in_features=32, out_features=16, bias=True)\n",
       "        (5): Linear(in_features=16, out_features=1, bias=True)\n",
       "        (6): Flatten(start_dim=0, end_dim=-1)\n",
       "      )\n",
       "    )\n",
       "    (lodge_kudzu_20250312): Sequential(\n",
       "      (0): Sequential(\n",
       "        (0): Unflatten(dim=1, unflattened_size=[1, 512])\n",
       "        (1): Conv2d(1, 1, kernel_size=(5, 5), stride=(2, 2))\n",
       "        (2): Flatten(start_dim=1, end_dim=2)\n",
       "        (3): Mish()\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): Conv1d(254, 256, kernel_size=(5,), stride=(2,))\n",
       "        (1): Mish()\n",
       "        (2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (3): Transpose()\n",
       "        (4): SelfAttention(\n",
       "          (attention_layer): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (5): Transpose()\n",
       "      )\n",
       "      (2): Sequential(\n",
       "        (0): Conv1d(256, 64, kernel_size=(5,), stride=(2,))\n",
       "        (1): Mish()\n",
       "        (2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (3): Transpose()\n",
       "        (4): SelfAttention(\n",
       "          (attention_layer): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (5): Transpose()\n",
       "      )\n",
       "      (3): Sequential(\n",
       "        (0): AdaptiveAvgPool1d(output_size=3)\n",
       "        (1): Conv1d(64, 32, kernel_size=(3,), stride=(1,))\n",
       "        (2): Flatten(start_dim=-2, end_dim=-1)\n",
       "        (3): Mish()\n",
       "        (4): Linear(in_features=32, out_features=16, bias=True)\n",
       "        (5): Linear(in_features=16, out_features=1, bias=True)\n",
       "        (6): Flatten(start_dim=0, end_dim=-1)\n",
       "      )\n",
       "    )\n",
       "    (merry_reef_20250312): Sequential(\n",
       "      (0): Sequential(\n",
       "        (0): Unflatten(dim=1, unflattened_size=[1, 512])\n",
       "        (1): Conv2d(1, 1, kernel_size=(5, 5), stride=(2, 2))\n",
       "        (2): Flatten(start_dim=1, end_dim=2)\n",
       "        (3): Mish()\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): Conv1d(254, 256, kernel_size=(5,), stride=(2,))\n",
       "        (1): Mish()\n",
       "        (2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (3): Transpose()\n",
       "        (4): SelfAttention(\n",
       "          (attention_layer): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (5): Transpose()\n",
       "      )\n",
       "      (2): Sequential(\n",
       "        (0): Conv1d(256, 64, kernel_size=(5,), stride=(2,))\n",
       "        (1): Mish()\n",
       "        (2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (3): Transpose()\n",
       "        (4): SelfAttention(\n",
       "          (attention_layer): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (5): Transpose()\n",
       "      )\n",
       "      (3): Sequential(\n",
       "        (0): AdaptiveAvgPool1d(output_size=3)\n",
       "        (1): Conv1d(64, 32, kernel_size=(3,), stride=(1,))\n",
       "        (2): Flatten(start_dim=-2, end_dim=-1)\n",
       "        (3): Mish()\n",
       "        (4): Linear(in_features=32, out_features=16, bias=True)\n",
       "        (5): Linear(in_features=16, out_features=1, bias=True)\n",
       "        (6): Flatten(start_dim=0, end_dim=-1)\n",
       "      )\n",
       "    )\n",
       "    (natural_buyout_20250312): Sequential(\n",
       "      (0): Sequential(\n",
       "        (0): Unflatten(dim=1, unflattened_size=[1, 512])\n",
       "        (1): Conv2d(1, 1, kernel_size=(5, 5), stride=(2, 2))\n",
       "        (2): Flatten(start_dim=1, end_dim=2)\n",
       "        (3): Mish()\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): Conv1d(254, 256, kernel_size=(5,), stride=(2,))\n",
       "        (1): Mish()\n",
       "        (2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (3): Transpose()\n",
       "        (4): SelfAttention(\n",
       "          (attention_layer): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (5): Transpose()\n",
       "      )\n",
       "      (2): Sequential(\n",
       "        (0): Conv1d(256, 64, kernel_size=(5,), stride=(2,))\n",
       "        (1): Mish()\n",
       "        (2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (3): Transpose()\n",
       "        (4): SelfAttention(\n",
       "          (attention_layer): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (5): Transpose()\n",
       "      )\n",
       "      (3): Sequential(\n",
       "        (0): AdaptiveAvgPool1d(output_size=3)\n",
       "        (1): Conv1d(64, 32, kernel_size=(3,), stride=(1,))\n",
       "        (2): Flatten(start_dim=-2, end_dim=-1)\n",
       "        (3): Mish()\n",
       "        (4): Linear(in_features=32, out_features=16, bias=True)\n",
       "        (5): Linear(in_features=16, out_features=1, bias=True)\n",
       "        (6): Flatten(start_dim=0, end_dim=-1)\n",
       "      )\n",
       "    )\n",
       "    (nila_ko_20250312): Sequential(\n",
       "      (0): Sequential(\n",
       "        (0): Unflatten(dim=1, unflattened_size=[1, 512])\n",
       "        (1): Conv2d(1, 1, kernel_size=(5, 5), stride=(2, 2))\n",
       "        (2): Flatten(start_dim=1, end_dim=2)\n",
       "        (3): Mish()\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): Conv1d(254, 256, kernel_size=(5,), stride=(2,))\n",
       "        (1): Mish()\n",
       "        (2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (3): Transpose()\n",
       "        (4): SelfAttention(\n",
       "          (attention_layer): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (5): Transpose()\n",
       "      )\n",
       "      (2): Sequential(\n",
       "        (0): Conv1d(256, 64, kernel_size=(5,), stride=(2,))\n",
       "        (1): Mish()\n",
       "        (2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (3): Transpose()\n",
       "        (4): SelfAttention(\n",
       "          (attention_layer): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (5): Transpose()\n",
       "      )\n",
       "      (3): Sequential(\n",
       "        (0): AdaptiveAvgPool1d(output_size=3)\n",
       "        (1): Conv1d(64, 32, kernel_size=(3,), stride=(1,))\n",
       "        (2): Flatten(start_dim=-2, end_dim=-1)\n",
       "        (3): Mish()\n",
       "        (4): Linear(in_features=32, out_features=16, bias=True)\n",
       "        (5): Linear(in_features=16, out_features=1, bias=True)\n",
       "        (6): Flatten(start_dim=0, end_dim=-1)\n",
       "      )\n",
       "    )\n",
       "    (rani_alon_20250302): Sequential(\n",
       "      (0): Sequential(\n",
       "        (0): Unflatten(dim=1, unflattened_size=[1, 512])\n",
       "        (1): Conv2d(1, 1, kernel_size=(5, 5), stride=(2, 2))\n",
       "        (2): Flatten(start_dim=1, end_dim=2)\n",
       "        (3): Mish()\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): Conv1d(254, 256, kernel_size=(5,), stride=(2,))\n",
       "        (1): Mish()\n",
       "        (2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (3): Transpose()\n",
       "        (4): SelfAttention(\n",
       "          (attention_layer): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (5): Transpose()\n",
       "      )\n",
       "      (2): Sequential(\n",
       "        (0): Conv1d(256, 64, kernel_size=(5,), stride=(2,))\n",
       "        (1): Mish()\n",
       "        (2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (3): Transpose()\n",
       "        (4): SelfAttention(\n",
       "          (attention_layer): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (5): Transpose()\n",
       "      )\n",
       "      (3): Sequential(\n",
       "        (0): AdaptiveAvgPool1d(output_size=3)\n",
       "        (1): Conv1d(64, 32, kernel_size=(3,), stride=(1,))\n",
       "        (2): Flatten(start_dim=-2, end_dim=-1)\n",
       "        (3): Mish()\n",
       "        (4): Linear(in_features=32, out_features=16, bias=True)\n",
       "        (5): Linear(in_features=16, out_features=1, bias=True)\n",
       "        (6): Flatten(start_dim=0, end_dim=-1)\n",
       "      )\n",
       "    )\n",
       "    (sourabh_katare_20250312): Sequential(\n",
       "      (0): Sequential(\n",
       "        (0): Unflatten(dim=1, unflattened_size=[1, 512])\n",
       "        (1): Conv2d(1, 1, kernel_size=(5, 5), stride=(2, 2))\n",
       "        (2): Flatten(start_dim=1, end_dim=2)\n",
       "        (3): Mish()\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): Conv1d(254, 256, kernel_size=(5,), stride=(2,))\n",
       "        (1): Mish()\n",
       "        (2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (3): Transpose()\n",
       "        (4): SelfAttention(\n",
       "          (attention_layer): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (5): Transpose()\n",
       "      )\n",
       "      (2): Sequential(\n",
       "        (0): Conv1d(256, 64, kernel_size=(5,), stride=(2,))\n",
       "        (1): Mish()\n",
       "        (2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (3): Transpose()\n",
       "        (4): SelfAttention(\n",
       "          (attention_layer): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (5): Transpose()\n",
       "      )\n",
       "      (3): Sequential(\n",
       "        (0): AdaptiveAvgPool1d(output_size=3)\n",
       "        (1): Conv1d(64, 32, kernel_size=(3,), stride=(1,))\n",
       "        (2): Flatten(start_dim=-2, end_dim=-1)\n",
       "        (3): Mish()\n",
       "        (4): Linear(in_features=32, out_features=16, bias=True)\n",
       "        (5): Linear(in_features=16, out_features=1, bias=True)\n",
       "        (6): Flatten(start_dim=0, end_dim=-1)\n",
       "      )\n",
       "    )\n",
       "    (tributary_glowing_20250312): Sequential(\n",
       "      (0): Sequential(\n",
       "        (0): Unflatten(dim=1, unflattened_size=[1, 512])\n",
       "        (1): Conv2d(1, 1, kernel_size=(5, 5), stride=(2, 2))\n",
       "        (2): Flatten(start_dim=1, end_dim=2)\n",
       "        (3): Mish()\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): Conv1d(254, 256, kernel_size=(5,), stride=(2,))\n",
       "        (1): Mish()\n",
       "        (2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (3): Transpose()\n",
       "        (4): SelfAttention(\n",
       "          (attention_layer): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (5): Transpose()\n",
       "      )\n",
       "      (2): Sequential(\n",
       "        (0): Conv1d(256, 64, kernel_size=(5,), stride=(2,))\n",
       "        (1): Mish()\n",
       "        (2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (3): Transpose()\n",
       "        (4): SelfAttention(\n",
       "          (attention_layer): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (5): Transpose()\n",
       "      )\n",
       "      (3): Sequential(\n",
       "        (0): AdaptiveAvgPool1d(output_size=3)\n",
       "        (1): Conv1d(64, 32, kernel_size=(3,), stride=(1,))\n",
       "        (2): Flatten(start_dim=-2, end_dim=-1)\n",
       "        (3): Mish()\n",
       "        (4): Linear(in_features=32, out_features=16, bias=True)\n",
       "        (5): Linear(in_features=16, out_features=1, bias=True)\n",
       "        (6): Flatten(start_dim=0, end_dim=-1)\n",
       "      )\n",
       "    )\n",
       "    (truss_odious_20250312): Sequential(\n",
       "      (0): Sequential(\n",
       "        (0): Unflatten(dim=1, unflattened_size=[1, 512])\n",
       "        (1): Conv2d(1, 1, kernel_size=(5, 5), stride=(2, 2))\n",
       "        (2): Flatten(start_dim=1, end_dim=2)\n",
       "        (3): Mish()\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): Conv1d(254, 256, kernel_size=(5,), stride=(2,))\n",
       "        (1): Mish()\n",
       "        (2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (3): Transpose()\n",
       "        (4): SelfAttention(\n",
       "          (attention_layer): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (5): Transpose()\n",
       "      )\n",
       "      (2): Sequential(\n",
       "        (0): Conv1d(256, 64, kernel_size=(5,), stride=(2,))\n",
       "        (1): Mish()\n",
       "        (2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (3): Transpose()\n",
       "        (4): SelfAttention(\n",
       "          (attention_layer): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (5): Transpose()\n",
       "      )\n",
       "      (3): Sequential(\n",
       "        (0): AdaptiveAvgPool1d(output_size=3)\n",
       "        (1): Conv1d(64, 32, kernel_size=(3,), stride=(1,))\n",
       "        (2): Flatten(start_dim=-2, end_dim=-1)\n",
       "        (3): Mish()\n",
       "        (4): Linear(in_features=32, out_features=16, bias=True)\n",
       "        (5): Linear(in_features=16, out_features=1, bias=True)\n",
       "        (6): Flatten(start_dim=0, end_dim=-1)\n",
       "      )\n",
       "    )\n",
       "    (university_toasted_20250312): Sequential(\n",
       "      (0): Sequential(\n",
       "        (0): Unflatten(dim=1, unflattened_size=[1, 512])\n",
       "        (1): Conv2d(1, 1, kernel_size=(5, 5), stride=(2, 2))\n",
       "        (2): Flatten(start_dim=1, end_dim=2)\n",
       "        (3): Mish()\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): Conv1d(254, 256, kernel_size=(5,), stride=(2,))\n",
       "        (1): Mish()\n",
       "        (2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (3): Transpose()\n",
       "        (4): SelfAttention(\n",
       "          (attention_layer): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (5): Transpose()\n",
       "      )\n",
       "      (2): Sequential(\n",
       "        (0): Conv1d(256, 64, kernel_size=(5,), stride=(2,))\n",
       "        (1): Mish()\n",
       "        (2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (3): Transpose()\n",
       "        (4): SelfAttention(\n",
       "          (attention_layer): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (5): Transpose()\n",
       "      )\n",
       "      (3): Sequential(\n",
       "        (0): AdaptiveAvgPool1d(output_size=3)\n",
       "        (1): Conv1d(64, 32, kernel_size=(3,), stride=(1,))\n",
       "        (2): Flatten(start_dim=-2, end_dim=-1)\n",
       "        (3): Mish()\n",
       "        (4): Linear(in_features=32, out_features=16, bias=True)\n",
       "        (5): Linear(in_features=16, out_features=1, bias=True)\n",
       "        (6): Flatten(start_dim=0, end_dim=-1)\n",
       "      )\n",
       "    )\n",
       "    (viva_press_20250312): Sequential(\n",
       "      (0): Sequential(\n",
       "        (0): Unflatten(dim=1, unflattened_size=[1, 512])\n",
       "        (1): Conv2d(1, 1, kernel_size=(5, 5), stride=(2, 2))\n",
       "        (2): Flatten(start_dim=1, end_dim=2)\n",
       "        (3): Mish()\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): Conv1d(254, 256, kernel_size=(5,), stride=(2,))\n",
       "        (1): Mish()\n",
       "        (2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (3): Transpose()\n",
       "        (4): SelfAttention(\n",
       "          (attention_layer): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (5): Transpose()\n",
       "      )\n",
       "      (2): Sequential(\n",
       "        (0): Conv1d(256, 64, kernel_size=(5,), stride=(2,))\n",
       "        (1): Mish()\n",
       "        (2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (3): Transpose()\n",
       "        (4): SelfAttention(\n",
       "          (attention_layer): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (5): Transpose()\n",
       "      )\n",
       "      (3): Sequential(\n",
       "        (0): AdaptiveAvgPool1d(output_size=3)\n",
       "        (1): Conv1d(64, 32, kernel_size=(3,), stride=(1,))\n",
       "        (2): Flatten(start_dim=-2, end_dim=-1)\n",
       "        (3): Mish()\n",
       "        (4): Linear(in_features=32, out_features=16, bias=True)\n",
       "        (5): Linear(in_features=16, out_features=1, bias=True)\n",
       "        (6): Flatten(start_dim=0, end_dim=-1)\n",
       "      )\n",
       "    )\n",
       "    (wendy_tan_20250312): Sequential(\n",
       "      (0): Sequential(\n",
       "        (0): Unflatten(dim=1, unflattened_size=[1, 512])\n",
       "        (1): Conv2d(1, 1, kernel_size=(5, 5), stride=(2, 2))\n",
       "        (2): Flatten(start_dim=1, end_dim=2)\n",
       "        (3): Mish()\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): Conv1d(254, 256, kernel_size=(5,), stride=(2,))\n",
       "        (1): Mish()\n",
       "        (2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (3): Transpose()\n",
       "        (4): SelfAttention(\n",
       "          (attention_layer): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (5): Transpose()\n",
       "      )\n",
       "      (2): Sequential(\n",
       "        (0): Conv1d(256, 64, kernel_size=(5,), stride=(2,))\n",
       "        (1): Mish()\n",
       "        (2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (3): Transpose()\n",
       "        (4): SelfAttention(\n",
       "          (attention_layer): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (5): Transpose()\n",
       "      )\n",
       "      (3): Sequential(\n",
       "        (0): AdaptiveAvgPool1d(output_size=3)\n",
       "        (1): Conv1d(64, 32, kernel_size=(3,), stride=(1,))\n",
       "        (2): Flatten(start_dim=-2, end_dim=-1)\n",
       "        (3): Mish()\n",
       "        (4): Linear(in_features=32, out_features=16, bias=True)\n",
       "        (5): Linear(in_features=16, out_features=1, bias=True)\n",
       "        (6): Flatten(start_dim=0, end_dim=-1)\n",
       "      )\n",
       "    )\n",
       "    (yonatan_wexler_20250302): Sequential(\n",
       "      (0): Sequential(\n",
       "        (0): Unflatten(dim=1, unflattened_size=[1, 512])\n",
       "        (1): Conv2d(1, 1, kernel_size=(5, 5), stride=(2, 2))\n",
       "        (2): Flatten(start_dim=1, end_dim=2)\n",
       "        (3): Mish()\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): Conv1d(254, 256, kernel_size=(5,), stride=(2,))\n",
       "        (1): Mish()\n",
       "        (2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (3): Transpose()\n",
       "        (4): SelfAttention(\n",
       "          (attention_layer): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (5): Transpose()\n",
       "      )\n",
       "      (2): Sequential(\n",
       "        (0): Conv1d(256, 64, kernel_size=(5,), stride=(2,))\n",
       "        (1): Mish()\n",
       "        (2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (3): Transpose()\n",
       "        (4): SelfAttention(\n",
       "          (attention_layer): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (5): Transpose()\n",
       "      )\n",
       "      (3): Sequential(\n",
       "        (0): AdaptiveAvgPool1d(output_size=3)\n",
       "        (1): Conv1d(64, 32, kernel_size=(3,), stride=(1,))\n",
       "        (2): Flatten(start_dim=-2, end_dim=-1)\n",
       "        (3): Mish()\n",
       "        (4): Linear(in_features=32, out_features=16, bias=True)\n",
       "        (5): Linear(in_features=16, out_features=1, bias=True)\n",
       "        (6): Flatten(start_dim=0, end_dim=-1)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (loss): BCEWithLogitsLoss()\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_router_model = multi_router_model.eval()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4819ce06-62f1-4056-996d-6e116811c21e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [a, b]\n",
       "Index: []"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame([dict(a=1, b='B'), dict(a=10, b='BBB')]).iloc[:0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edde7fe0-c2ac-4b4e-9123-c5f6ab7fa68f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nemoenv",
   "language": "python",
   "name": "nemoenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
